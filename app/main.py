import os
from dotenv import load_dotenv

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
env_path = os.path.join(project_root, ".env")
load_dotenv(dotenv_path=env_path)

import json
import time
import redis
import shutil
from typing import List
from datetime import date
from contextlib import asynccontextmanager

from fastapi import FastAPI, BackgroundTasks, Depends, status, Form, File, UploadFile, HTTPException, Header, Query
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.ext.asyncio import AsyncSession
from faster_whisper import WhisperModel

from app.graph.workflow import app_graph
from app.core.semantic_cache import SemanticCacheManager
from app.core.dependencies import check_access_token
from app.core.database import get_db
from app.utils.file_parser import parse_uploaded_file
from app.services.member import MemberService
from app.services import announcement_service, mail_service, meeting_service 
from app.services.memory import ConversationMemoryManager
from app.schemas.model import (
    Member, MemberResponse, LoginRequest, LoginResponse, MemberInfo, ChatRequest,
    AnnouncementListResponse, AnnouncementCreateRequest, AnnouncementDetailResponse,
    MailSendRequest, MailListResponse, AddressBookResponse, MonthlyReservationResponse,
    DailyReservationResponseItem, ReservationCreateRequest, ReservationCancelRequest
)

# Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ìºì‹œ, ë©”ëª¨ë¦¬ ê³µìœ 
redis_client = redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379"), decode_responses=False)

# Redis ì„¤ì • ë° Semantic Cache ë§¤ë‹ˆì € ì´ˆê¸°í™”
semantic_cache: SemanticCacheManager = None
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    global semantic_cache
    semantic_cache = SemanticCacheManager(redis_client)
    yield

# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ë²„í¼ ì„¤ì •    
memory_manager = ConversationMemoryManager(redis_client, window_size=30)    
    
# ì•±ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ëª¨ë“  ìš”ì²­ì— ëŒ€í•´, check_access_tokenì„ ì‹¤í–‰í•˜ì—¬ accessToken ê²€ì¦
app = FastAPI(lifespan=lifespan, dependencies=[Depends(check_access_token)], title="Deep Nexus AI Backend")  

origins = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],  # í•µì‹¬: ëª¨ë“  ë©”ì„œë“œ(OPTIONS í¬í•¨) í—ˆìš©
    allow_headers=["*"],  # í•µì‹¬: ëª¨ë“  í—¤ë” í—ˆìš©
)


@app.post("/signup", response_model=MemberResponse, status_code=status.HTTP_201_CREATED)
async def signup(request: Member, db: AsyncSession = Depends(get_db)):
    """
    íšŒì›ê°€ì…
    """
    member_service = MemberService(db)
    result = await member_service.create_employee(request)
    
    return MemberResponse(
        message="íšŒì›ê°€ì…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    )
    
@app.post("/login", response_model=LoginResponse, status_code=status.HTTP_200_OK)
async def login(request: LoginRequest, db: AsyncSession = Depends(get_db)):
    """
    ì´ë©”ì¼ ë¡œê·¸ì¸
    """
    member_service = MemberService(db)
    return await member_service.login_employee(request)    


@app.post("/refresh", status_code=status.HTTP_200_OK)
async def refresh_token(
    member_request: MemberInfo,                         # Body : ì¬ë°œê¸‰ì— í•„ìš”í•œ íšŒì› ì •ë³´
    refresh_token: str = Header(alias="refreshToken"),  # Header: ë¦¬í”„ë ˆì‹œ í† í°
    db: AsyncSession = Depends(get_db)
):
    """
    AccessToken ì¬ë°œê¸‰
    """
    member_service = MemberService(db)
    
    # í† í° ì¬ë°œê¸‰ ì²˜ë¦¬
    return await member_service.refresh_access_token(refresh_token, member_request)


# ë¹„ë™ê¸° ì²˜ë¦¬í•˜ì—¬, I/O(DB, LLM, Redis)ì‘ì—… ì‹œ, ì„œë²„ê°€ ë¸”ë¡œí‚¹ ë˜ì§€ ì•Šë„ë¡ í•¨.
@app.post("/chat")
async def chat_endpoint(
    file: UploadFile = File(None), # ì—…ë¡£ ã…¡íŒŒì¼
    request_data: str = Form(...), # ì±„íŒ… ì‹œ, ìš”ì²­í•œ íŒŒë¼ë¯¸í„°
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    start_time = time.perf_counter()
    
    # ì‚¬ìš©ì ì—…ë¡œë“œ íŒŒì¼ ì²˜ë¦¬
    try:
        data_dict = json.loads(request_data)
        request = ChatRequest(**data_dict)
    except Exception as e:
        raise HTTPException(status_code=422, detail=f"Invalid JSON format: {str(e)}")
    
    file_context_str = ""
    if file:
        print(f"ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file.filename}")
        file_context_str = await parse_uploaded_file(file)
        print(f"ğŸ“„ ì¶”ì¶œëœ ë‚´ìš©: {file_context_str[:100]}...")
    
    # Redis ìºì‹œ ì •ë³´ í™•ì¸     
    cached_response = await semantic_cache.search_cache(request.query)
    
    if cached_response:
        # ìºì‹œ íˆíŠ¸ ì‹œ: ì €ì¥ëœ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        async def cached_stream_generator():
            print(f"\n [Semantic Cache Hit] > ", end="", flush=True)
            yield cached_response

        return StreamingResponse(cached_stream_generator(), media_type="text/event-stream")

    # Redis Memory ë²„í¼ ì—ì„œ, ì´ì „ ëŒ€í™” ë‚´ìš© ì½ì–´ì˜¤ê¸° 
    history = await memory_manager.get_history(request.employee_id)
    
    # Graph Execution & Streaming
    async def event_generator():
        final_output = ""
        is_first_chunk = True
        
        # LangGraph astream_events ì‚¬ìš©í•˜ì—¬ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°
        inputs = {
            "question": request.query,
            "employee_id": request.employee_id,
            "job_rank_id" : request.job_rank_id,
            "department_code": request.department_code,
            "parent_department" : request.parent_department,
            "company_email" : request.company_email,
            "file_context": file_context_str,
            "history" : history
        }
        
        # app.graph.workflow - LangGraph ì‹¤í–‰
        async for event in app_graph.astream_events(inputs, version="v1"):
            kind = event["event"]
            node_name = event["metadata"].get("langgraph_node", "Unknown")
            
            print(f"event : {event}")
            # ------------------------------------------------------------------
            # 1. [ë””ë²„ê¹…] ë…¸ë“œ ì§„ì…/ì™„ë£Œ í™•ì¸ (Router -> SQL Agent -> ...)
            # ------------------------------------------------------------------
            if kind == "on_chain_start" and node_name in ["router", "sql_agent", "vector_search", "generator"]:
                print(f"  ğŸ”„ [Node Start] {node_name} ë…¸ë“œ ì§„ì…...")
                
            elif kind == "on_chain_end" and node_name in ["router", "sql_agent", "vector_search"]:
                # ê° ë…¸ë“œê°€ ë±‰ì–´ë‚¸ ê²°ê³¼ê°’(Output) í™•ì¸
                output_data = event["data"].get("output")
                if output_data:
                    # ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
                    print(f"  âœ… [Node End] {node_name} ì™„ë£Œ. ê²°ê³¼: {str(output_data)[:100]}...")

            # ------------------------------------------------------------------
            # 2. [ë””ë²„ê¹…] Routerì˜ íŒë‹¨ ê²°ê³¼ í™•ì¸
            # ------------------------------------------------------------------
            if kind == "on_chain_end" and node_name == "router":
                router_output = event["data"]["output"]
                print(f"     ğŸ‘‰ Router íŒë‹¨: {router_output})")

            # ------------------------------------------------------------------
            # 3. [ë””ë²„ê¹…] íˆ´ ì‹¤í–‰ ê²°ê³¼ í™•ì¸ (SQL ì¿¼ë¦¬ ê²°ê³¼, ê²€ìƒ‰ëœ ë¬¸ì„œ ë“±)
            # ------------------------------------------------------------------
            if kind == "on_tool_end":
                tool_name = event["name"]
                tool_output = event["data"].get("output")
                print(f"     ğŸ› ï¸ [Tool] {tool_name} ì‹¤í–‰ ì™„ë£Œ.")
                # SQL ì¿¼ë¦¬ ê²°ê³¼ë‚˜ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                print(f"        ê²°ê³¼: {str(tool_output)[:150]}...")
            
            # ------------------------------------------------------------------
            # 4. [í´ë¼ì´ì–¸íŠ¸ ì „ì†¡] ìµœì¢… ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° (ê¸°ì¡´ ë¡œì§)
            # ------------------------------------------------------------------
            if kind == "on_chat_model_stream" and node_name == "generator":
                content = event["data"]["chunk"].content
                if content:
                    if is_first_chunk:
                        print(f"\n ğŸ’¬ [Streaming Start] >> ", end="", flush=True)
                        is_first_chunk = False
                    
                    # 1. í„°ë¯¸ë„ ë¡œê·¸ì— ì‹¤ì‹œê°„ ì¶œë ¥ (ì¤„ë°”ê¿ˆ ì—†ì´)
                    print(content, end="", flush=True)
                    
                    # 2. ë°ì´í„° ëˆ„ì 
                    final_output += content
                    
                    # 3. í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡
                    yield content
                    
        end_time = time.perf_counter()
        total_duration = end_time - start_time            
        
        if not is_first_chunk:
            print("\n âœ… [Streaming End] : Total Runtime {:.2f} seconds".format(total_duration))
            
        # Redis ìºì‹œ ì €ì¥(ë§Œë£Œ ì‹œê°„ 1ì‹œê°„) ë° ë©”ëª¨ë¦¬ì— ëŒ€í™” ë‚´ìš© ê¸°ë¡
        if final_output:
            background_tasks.add_task(
                semantic_cache.store_cache, 
                query_text=request.query, 
                response_text=final_output
            )
            background_tasks.add_task(
                memory_manager.add_message, 
                session_id=request.employee_id, 
                role="user", 
                content=request.query
            )
            background_tasks.add_task(
                memory_manager.add_message, 
                session_id=request.employee_id, 
                role="assistant", 
                content=final_output
            )

    
    return StreamingResponse(event_generator(), media_type="text/event-stream")


@app.get("/announcements", response_model=List[AnnouncementListResponse])
async def read_announcements(
    parent_department_code: str = Query(..., description="ìƒìœ„ ë¶€ì„œ ì½”ë“œ"),
    employee_id: str = Query(..., description="ì‚¬ì› ID"),
    db: AsyncSession = Depends(get_db)
):
    """ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ"""
    return await announcement_service.get_announcement_list(db, parent_department_code, employee_id)
@app.post("/announcements")
async def create_announcement_item(
    request: AnnouncementCreateRequest,
    db: AsyncSession = Depends(get_db)
):
    """ê³µì§€ì‚¬í•­ ì‘ì„± ìš”ì²­"""
    return await announcement_service.create_announcement(db, request)

@app.get("/announcements/{announcement_id}", response_model=AnnouncementDetailResponse)
async def read_announcement_detail(
    announcement_id: int,
    employee_id: str,
    db: AsyncSession = Depends(get_db)
):
    """ê³µì§€ì‚¬í•­ ìƒì„¸ë³´ê¸° ìš”ì²­"""
    return await announcement_service.get_announcement_detail(db, announcement_id, employee_id)
@app.post("/mails/send")
async def send_mail(
    request: MailSendRequest,
    db: AsyncSession = Depends(get_db)
):
    """ë©”ì¼ ì „ì†¡"""
    return await mail_service.send_email_logic(db, request)

@app.get("/mails/sent", response_model=List[MailListResponse])
async def read_sent_mails(
    sender_email: str = Query(..., description="ë°œì‹ ì ì´ë©”ì¼"),
    db: AsyncSession = Depends(get_db)
):
    """ë³´ë‚¸ ë©”ì¼í•¨ ëª©ë¡ ì¡°íšŒ"""
    return await mail_service.get_sent_mails(db, sender_email)

@app.delete("/mails/{mail_id}")
async def delete_sent_mails(
    mail_id: int,
    sender_email: str = Query(..., description="ë°œì‹ ì ì´ë©”ì¼"),
    db: AsyncSession = Depends(get_db)
):
    """ë³´ë‚¸ ë©”ì¼í•¨ ì‚­ì œ"""
    return await mail_service.delete_sent_mails(db, mail_id, sender_email)

@app.get("/address-book", response_model=List[AddressBookResponse])
async def read_address_book(
    db: AsyncSession = Depends(get_db)
):
    """ì£¼ì†Œë¡ ëª©ë¡ ì¡°íšŒ"""
    return await mail_service.get_address_book(db)


@app.get("/meeting-rooms/reservations/monthly", response_model=MonthlyReservationResponse)
async def get_monthly_reservations(
    meeting_room_id: str = Query(..., description="ë¯¸íŒ…ë£¸ ID"),
    year_month: str = Query(..., description="ì¡°íšŒ ì›” (YYYY-MM)"),
    employee_id:str=Query(..., description="ì‚¬ì› ID"),
    db: AsyncSession = Depends(get_db)
):
    """ì¼ë³„ ì˜ˆì•½ í˜„í™© ì¡°íšŒ"""
    return await meeting_service.get_monthly_status(db, meeting_room_id, year_month, employee_id)

@app.get("/meeting-rooms/reservations/daily", response_model=List[DailyReservationResponseItem])
async def get_daily_reservations(
    meeting_room_id: str = Query(..., description="ë¯¸íŒ…ë£¸ ID"),
    usage_date: date = Query(..., description="ì¡°íšŒ ì¼ì (YYYY-MM-DD)"),
    employee_id:str=Query(..., description="ì‚¬ì› ID"),
    db: AsyncSession = Depends(get_db)
):
    """ì¼, ì‹œê°„ë³„ ì˜ˆì•½ í˜„í™© ì¡°íšŒ"""
    return await meeting_service.get_daily_status(db, meeting_room_id, usage_date, employee_id)

@app.post("/meeting-rooms/reservations")
async def create_reservation(
    request: ReservationCreateRequest,
    db: AsyncSession = Depends(get_db)
):
    """íšŒì˜ì‹¤ ì˜ˆì•½ ìš”ì²­"""
    print("TEST")
    return await meeting_service.create_reservation(db, request)

@app.delete("/meeting-rooms/reservations")
async def delete_reservation(
    request: ReservationCancelRequest,
    db: AsyncSession = Depends(get_db)
):
    """íšŒì˜ì‹¤ ì˜ˆì•½ ì·¨ì†Œ ìš”ì²­"""
    return await meeting_service.delete_reservation(db, request.reservation_id_list, request.employee_id)




# ëª¨ë¸ í¬ê¸° ì˜µì…˜: "tiny", "base", "small", "medium", "large-v3"
# device="cuda" (GPU ì‚¬ìš© ì‹œ), device="cpu" (CPU ì‚¬ìš© ì‹œ)
# compute_type="float16" (GPU), compute_type="int8" (CPU)
MODEL_SIZE = "medium" 
try:
    # GPUê°€ ìˆìœ¼ë©´ GPUë¡œ, ì—†ìœ¼ë©´ CPUë¡œ ì„¤ì •í•˜ëŠ” ë¡œì§ (ì›í•˜ëŠ” ëŒ€ë¡œ ê³ ì •í•´ë„ ë¨)
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"
except ImportError:
    device = "cpu"
    compute_type = "int8"

print(f"Loading Faster Whisper model '{MODEL_SIZE}' on {device} with {compute_type}...")
model = WhisperModel(MODEL_SIZE, device=device, compute_type=compute_type)
print("Model loaded successfully.")

@app.post("/stt")
async def speech_to_text(file: UploadFile = File(...)):
    """
    í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ Faster-Whisperë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    """
    
    # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
    temp_filename = f"temp_{file.filename}"
    
    try:
        # 3. ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¡œì»¬ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        # faster-whisperëŠ” íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥ë°›ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.
        with open(temp_filename, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # 4. Transcribe (ë³€í™˜) ìˆ˜í–‰
        # segmentsëŠ” ì œë„ˆë ˆì´í„°ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ë°˜ë³µë¬¸ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        segments, info = model.transcribe(temp_filename, beam_size=5, language="ko", vad_filter=True, temperature=0.0, condition_on_previous_text=False)
        
        # 5. ê²°ê³¼ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        result_text = "".join([segment.text for segment in segments]).strip()
        
        print(f"Detected language: {info.language}, Probability: {info.language_probability}")
        print(f"Transcription: {result_text}")

        return {"transcript": result_text}

    except Exception as e:
        print(f"STT Error: {e}")
        raise HTTPException(status_code=500, detail=f"STT processing failed: {str(e)}")
        
    finally:
        # 6. ì„ì‹œ íŒŒì¼ ì‚­ì œ (í´ë¦°ì—…)
        if os.path.exists(temp_filename):
            os.remove(temp_filename)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)