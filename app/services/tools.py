from sqlalchemy import text
from app.core.database import AsyncSessionLocal
from app.services.llm import get_embeddings
import json
from sentence_transformers import CrossEncoder
import re
import time
import torch
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer


# ì„ë² ë”©(ìì—°ì–´ => ë²¡í„°) ëª¨ë¸ : KURE-v1
embeddings = get_embeddings()

# Reranker(ìƒìœ„ 10ê°œ ë¬¸ì„œ ì¶”ì¶œ) ëª¨ë¸
ONNX_MODEL_DIR = "app/models/bge-reranker-onnx-int8"
onnx_tokenizer = AutoTokenizer.from_pretrained(ONNX_MODEL_DIR)
onnx_model = ORTModelForSequenceClassification.from_pretrained(
    ONNX_MODEL_DIR, 
    provider="CPUExecutionProvider"
)

# Text-to-SQLìš© í‚¤ì›Œë“œ(optimized_sql_keywords)ë¥¼ ë°”íƒ•ìœ¼ë¡œ RDB ìŠ¤í‚¤ë§ˆ ë²¡í„° ê²€ìƒ‰ í›„ DDL ì¶”ì¶œ
async def search_schema_and_get_ddl(query_text: str) -> str:
    # ìì—°ì–´ => ë²¡í„° ë³€í™˜
    query_vector = await embeddings.aembed_query(query_text)
    
    # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ 5ê°œ DDL ì¶”ì¶œ
    async with AsyncSessionLocal() as session:
        stmt = text("""
            SELECT ddl_content 
            FROM tbl_deep_nexus_schema
            ORDER BY schema_vector <=> :vector LIMIT 5
        """)
        result = await session.execute(stmt, {"vector": str(query_vector)})
        rows = result.fetchall()
        return "\n\n".join([row[0] for row in rows])

    
# SQL Injection ê³µê²© ì°¨ë‹¨ì„ ìœ„í•œ RLS ì»¨í…ìŠ¤íŠ¸ ê°’ ê²€ì¦
def validate_security_context(value: str | int) -> str:
    str_val = str(value)
    
    # ì˜ë¬¸ì, ìˆ«ì, ì–¸ë”ë°”, í•˜ì´í”ˆ ì™¸ì˜ ë¬¸ì ë°œê²¬ ì‹œ ì˜ˆì™¸ ë°œìƒ
    if not re.match(r"^[a-zA-Z0-9_\-]+$", str_val):
        raise ValueError(f"Security Alert: Invalid context value detected -> {str_val}")
    return str_val

# ìƒì„±ëœ SQL ì‹¤í–‰(RLS ì ìš©)
import json
from sqlalchemy import text

async def execute_sql_query(sql: str, employee_id: str, department_code: str, parent_department: str, job_rank_id: str) -> str:
    try:
        # 1. ì…ë ¥ê°’ ê²€ì¦ (SQL Injection ë°©ì§€)
        safe_employee = validate_security_context(employee_id)
        safe_dept = validate_security_context(department_code)
        safe_parent = validate_security_context(parent_department)
        safe_rank = validate_security_context(job_rank_id)
        
        #print(f"safe_employee : {safe_employee}, safe_dept : {safe_dept}, safe_parent : {safe_parent}, safe_rank : {safe_rank}")
        
    except ValueError as e:
        return json.dumps({"status": "error", "message": str(e)}, ensure_ascii=False)

    async with AsyncSessionLocal() as session:
        try:
            # 2. í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì‹¤í–‰ (SET LOCALì˜ ìœ íš¨ ë²”ìœ„ ë³´ì¥)
            async with session.begin():
                
                # 3. ë‹¤ì¤‘ RLS ì„¸ì…˜ ë³€ìˆ˜ ì£¼ì… (set_config í™œìš©)
                set_context_sql = text("""
                    SELECT 
                        set_config('app.current_employee_id', :emp, true),
                        set_config('app.current_dept_code', :dept, true),
                        set_config('app.current_parent_dept_code', :p_dept, true),
                        set_config('app.current_rank_level', :rank, true);
                """)
                
                await session.execute(set_context_sql, {
                    "emp": safe_employee,
                    "dept": safe_dept,
                    "p_dept": safe_parent,
                    "rank": safe_rank
                })
                
                # 4. LLMì´ ìƒì„±í•œ ë©”ì¸ SQL ì‹¤í–‰
                result = await session.execute(text(sql))
                columns = result.keys()
                rows = result.fetchall()
                
                # 5. ê²°ê³¼ê°€ 0ê±´ì¼ ê²½ìš° ê¶Œí•œ ì²´í¬
                if not rows:
                    # SELECT ì¿¼ë¦¬ì¸ ê²½ìš°ì—ë§Œ ì‰ë„ìš° ì¹´ìš´íŠ¸ ì§„í–‰ (DML ì‚¬ì´ë“œ ì´í™íŠ¸ ë°©ì§€)
                    if sql.strip().upper().startswith("SELECT"):
                        # LLMì´ ìƒì„±í•œ SQL ë§ˆì§€ë§‰ ì„¸ë¯¸ì½œë¡  ì œê±°
                        clean_sql = sql.strip().rstrip(';')
                        
                        shadow_check = await session.execute(
                            text("SELECT fn_check_query_count_bypass_rls(:query)"),
                            {"query": clean_sql}
                        )
                        shadow_count = shadow_check.scalar()
                        
                        if shadow_count > 0:
                            return json.dumps({
                                "status": "forbidden",
                                "message": "ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ, í˜„ì¬ ì‚¬ìš©ìì˜ ê¶Œí•œ(ì§ê¸‰/ë¶€ì„œ)ìœ¼ë¡œëŠ” ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            }, ensure_ascii=False)
                        else:
                            return json.dumps({
                                "status": "not_found",
                                "message": "ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” ë°ì´í„°ê°€ ì‹œìŠ¤í…œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                            }, ensure_ascii=False)
                    
                    return json.dumps([], ensure_ascii=False)

                # 6. ì •ìƒ ê²°ê³¼ ë°˜í™˜
                data = [dict(zip(columns, row)) for row in rows]
                return json.dumps(data, ensure_ascii=False, default=str)
                
        except Exception as e:
            return json.dumps({"status": "error", "message": f"SQL Execution Error: {str(e)}"}, ensure_ascii=False)

# ë¹„ì •í˜• ë°ì´í„°ì— ëŒ€í•œ í•˜ì´ë¸Œë¦¬ë“œ(í‚¤ì›Œë“œ + ë²¡í„°) ê²€ìƒ‰ ìˆ˜í–‰ -> ìƒìœ„ 5ê°œ ë¬¸ì„œ ë°˜í™˜
async def hybrid_vector_search(query_text: str, department_code: str, filter_keywords: list[str]) -> str:
    #total_start = time.perf_counter()
    
    # 1. ì„ë² ë”© ìƒì„±
    #step1_start = time.perf_counter()
    query_vector = await embeddings.aembed_query(query_text)
    #print(f"â±ï¸ [Step 1: Embedding] {time.perf_counter() - step1_start:.4f} sec")
    
    async with AsyncSessionLocal() as session:
        try:
            # 2. DB ì„¤ì • ìµœì í™”
            #step2_start = time.perf_counter()
            await session.execute(text("SET LOCAL jit = off"))
            
            #print(f"â±ï¸ [Step 2: DB Config] {time.perf_counter() - step2_start:.4f} sec")
            
            # 3. ë²¡í„° ê²€ìƒ‰ HNSW ì¸ë±ìŠ¤ ì‚¬ìš©
            #step3_start = time.perf_counter()
            vector_sql = text("""
                SELECT content, metadata, doc_url, doc_title, (1 - (content_vector <=> :vector)) as sim_score
                FROM tbl_deep_nexus_docs
                ORDER BY content_vector <=> :vector ASC
                LIMIT 15
            """)
            
            vector_result = await session.execute(vector_sql, {"vector": str(query_vector)})
            vector_rows = vector_result.fetchall()
            #print(f"â±ï¸ [Step 3: Vector Search] {time.perf_counter() - step3_start:.4f} sec (Found: {len(vector_rows)})")
            
            # 4. í‚¤ì›Œë“œ ê²€ìƒ‰ GIN ì¸ë±ìŠ¤ ì‚¬ìš©
            #step4_start = time.perf_counter()
            keyword_rows = []
            if filter_keywords:
                safe_keywords = [f"'%{kw.replace("'", "''")}%'" for kw in filter_keywords]
                array_literal = ", ".join(safe_keywords)
                
                keyword_sql = text(f"""
                    SELECT content, metadata, doc_url, doc_title,0.0 as sim_score
                    FROM tbl_deep_nexus_docs
                    WHERE content ILIKE ANY(ARRAY[{array_literal}])
                    LIMIT 15
                """)
                keyword_result = await session.execute(keyword_sql)
                keyword_rows = keyword_result.fetchall()
            #print(f"â±ï¸ [Step 4: Keyword Search] {time.perf_counter() - step4_start:.4f} sec (Found: {len(keyword_rows)})")
            
            # 5. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
            #step5_start = time.perf_counter()
            unique_docs = {}
            for row in vector_rows:
                unique_docs[row.content] = row
            for row in keyword_rows:
                if row.content not in unique_docs:
                    unique_docs[row.content] = row
            combined_rows = list(unique_docs.values())
            #print(f"â±ï¸ [Step 5: Merge] {time.perf_counter() - step5_start:.4f} sec (Total: {len(combined_rows)})")
            
            if not combined_rows:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

            # 6. Reranking
            #step6_start = time.perf_counter()
            documents = [row.content for row in combined_rows]
            pairs = [[query_text, doc] for doc in documents]
            
            # ONNX ì¶”ë¡  ë¡œì§ ì ìš©
            inputs = onnx_tokenizer(
                pairs, padding=True, truncation=True, return_tensors="pt", max_length=256
            )
            with torch.no_grad():
                outputs = onnx_model(**inputs)
                scores = outputs.logits.view(-1,).float().tolist()
            
            # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬
            scored_docs = sorted(zip(scores, combined_rows), key=lambda x: x[0], reverse=True)
            
            # ìƒìœ„ 3ê°œ ì„ íƒ
            top_k = scored_docs[:3]
            formatted_docs = []
            for score, row in top_k:
                formatted_docs.append(
                    f"- ë‚´ìš© : {row.content}\n- íŒŒì¼ëª… : {row.doc_title}\n- ì¶œì²˜ : {row.doc_url}\n"
                )
            #print(f"â±ï¸ [Step 6: Reranking (ONNX)] {time.perf_counter() - step6_start:.4f} sec")
            
            #print(f"ğŸš€ [Total Latency] {time.perf_counter() - total_start:.4f} sec")
            return "\n\n".join(formatted_docs)
            
        except Exception as e:
            return f"Error: {str(e)}"            