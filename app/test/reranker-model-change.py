# BAAI/bge-reranker-v2-m3 => ONNX ëª¨ë¸ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸. í•œë²ˆë§Œ ì‹¤í–‰í•  ê²ƒ.
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

model_id = "BAAI/bge-reranker-v2-m3"
save_dir = "app/models/bge-reranker-onnx"

model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)
tokenizer = AutoTokenizer.from_pretrained(model_id)

model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"âœ… 'logits'ê°€ í¬í•¨ëœ ëª¨ë¸ë¡œ ì¬ë³€í™˜ ì™„ë£Œ: {save_dir}")


# ONXX Reranking í…ŒìŠ¤íŠ¸ ì½”ë“œ
import time
import torch
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

model_dir = "app/models/bge-reranker-onnx"

print("ğŸš€ ìµœì í™”ëœ ONNX ëª¨ë¸ ë¡œë”© ì¤‘...")
# ê²½ê³  í•´ê²°ì„ ìœ„í•´ fix_mistral_regex=True ì¶”ê°€ (ì§€ì›í•˜ëŠ” ê²½ìš°)
tokenizer = AutoTokenizer.from_pretrained(model_dir) 
model = ORTModelForSequenceClassification.from_pretrained(model_dir, provider="CPUExecutionProvider")

pairs = [["ê²€ìƒ‰ì–´ ì˜ˆì‹œì…ë‹ˆë‹¤.", "ë¬¸ì„œ ë‚´ìš© ì˜ˆì‹œ 1ë²ˆì…ë‹ˆë‹¤."] for _ in range(16)]

print(f"â±ï¸ {len(pairs)}ê°œ ë¬¸ì„œ Reranking ì‹œì‘...")
start_time = time.perf_counter()

# í† í°í™”
inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)

with torch.no_grad():
    # ì´ì œ ëª¨ë¸ì€ 'logits'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    outputs = model(**inputs)
    # Rerankerì˜ ì ìˆ˜ëŠ” ë³´í†µ logitsì˜ ì²« ë²ˆì§¸ ê°’ì…ë‹ˆë‹¤.
    scores = outputs.logits.view(-1,).float().tolist()

end_time = time.perf_counter()
print(f"âœ… Reranking ì™„ë£Œ! ì†Œìš” ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")
print(f"ğŸ“Š ê²°ê³¼ ì ìˆ˜(ìƒ˜í”Œ): {scores[:3]}")


# 
from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_dir = "app/models/bge-reranker-onnx"
quantized_model_dir = "app/models/bge-reranker-onnx-int8"

# 1. ì–‘ìí™”ê¸° ì„¤ì •
quantizer = ORTQuantizer.from_pretrained(model_dir)
dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False) # CPU ìµœì í™” ì„¤ì •

# 2. ì–‘ìí™” ì‹¤í–‰ ë° ì €ì¥
quantizer.quantize(
    save_dir=quantized_model_dir,
    quantization_config=dqconfig,
)
print("âœ… INT8 ì–‘ìí™” ëª¨ë¸ ìƒì„± ì™„ë£Œ!")